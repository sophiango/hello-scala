# 🚀 Data Engineering & Analysis Practice with Apache Spark

Welcome to my **personal learning playground** for exploring **Data Engineering and Analytics** using:

- ⚡ **Apache Spark (Scala & SQL)**
- 🗂️ **Big Data concepts (ETL, Parquet, DAGs, window functions, etc.)**
- 📊 **Data analysis techniques** for building pipelines and insights

This repo is **not a polished project**, but rather a **hands-on lab** where I practice new concepts daily, experiment with Spark APIs, and document my learning journey.

---

## 📖 Goals of This Repo
✔️ Learn Spark internals (DAGs, lazy evaluation, transformations vs actions)  
✔️ Practice **DataFrame API** and **Spark SQL** side by side  
✔️ Work with real-world datasets (CSV, Parquet, JSON)  
✔️ Explore **data engineering workflows**: ingestion → cleaning → transformation → analysis  
✔️ Build intuition for performance (caching, partitioning, cluster execution)  
✔️ Strengthen **Scala coding** skills while practicing **SQL queries**

---

## 🛠️ Tech Stack

- **Apache Spark** (local mode for now)
- **Scala** (via sbt build tool)
- **Spark SQL** for relational-style analysis
- **Parquet & CSV** for dataset storage formats
- (Later) 💡 Potential to add: Airflow, dbt, Delta Lake

---

## 🏃 Running the Code
1. Clone the repo
```declarative
git clone git@github.com:sophiango/hello-scala.git
cd hello-scala
```
2. Run with sbt
```declarative
sbt run
```
---

## 🌟 Learning Journey
I’m using this repo as a daily practice log for:

- Day 1 → Spark setup, RDDs vs DataFrames, first DAGs 
- Day 2 → Filtering nulls, aggregations, Parquet intro 
- Day 3 → Window functions, ranking, explode() for genres
- (More coming soon...)

--- 

## 📌 Notes
This is a work-in-progress repo ✨ built for learning and experimentation.
It’s not production-ready but instead a showcase of my hands-on growth in data engineering and analytics.